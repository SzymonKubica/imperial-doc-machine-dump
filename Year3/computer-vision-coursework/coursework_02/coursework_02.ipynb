{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XRxHiKdGHiT"
   },
   "source": [
    "# Coursework 2: Image segmentation\n",
    "\n",
    "In this coursework you will develop and train a convolutional neural network for brain tumour image segmentation. Please read both the text and the code in this notebook to get an idea what you are expected to implement. Pay attention to the missing code blocks that look like this:\n",
    "\n",
    "```\n",
    "### Insert your code ###\n",
    "...\n",
    "### End of your code ###\n",
    "```\n",
    "\n",
    "## What to do?\n",
    "\n",
    "* Complete and run the code using `jupyter-lab` or `jupyter-notebook` to get the results.\n",
    "\n",
    "* Export (File | Save and Export Notebook As...) the notebook as a PDF file, which contains your code, results and answers, and upload the PDF file onto [Scientia](https://scientia.doc.ic.ac.uk).\n",
    "\n",
    "* Instead of clicking the Export button, you can also run the following command instead: `jupyter nbconvert coursework.ipynb --to pdf`\n",
    "\n",
    "* If Jupyter complains about some problems in exporting, it is likely that pandoc (https://pandoc.org/installing.html) or latex is not installed, or their paths have not been included. You can install the relevant libraries and retry.\n",
    "\n",
    "* If Jupyter-lab does not work for you at the end, you can use Google Colab to write the code and export the PDF file.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "You need to install Jupyter-Lab (https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html) and other libraries used in this coursework, such as by running the command:\n",
    "`pip3 install [package_name]`\n",
    "\n",
    "## GPU resource\n",
    "\n",
    "The coursework is developed to be able to run on CPU, as all images have been pre-processed to be 2D and of a smaller size, compared to original 3D volumes.\n",
    "\n",
    "However, to save training time, you may want to use GPU. In that case, you can run this notebook on Google Colab. On Google Colab, go to the menu, Runtime - Change runtime type, and select **GPU** as the hardware acceleartor. At the end, please still export everything and submit as a PDF file on Scientia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Eq1KWmR3HWYV"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv3\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Year3/Computer-Vision-Coursework/coursework_02/venv/lib/python3.10/site-packages/torch/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[1;32m      4\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[1;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "File \u001b[0;32m~/Documents/Year3/Computer-Vision-Coursework/coursework_02/venv/lib/python3.10/site-packages/torch/nn/modules/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Identity, Linear, Bilinear, LazyLinear\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1d, Conv2d, Conv3d, \\\n\u001b[1;32m      4\u001b[0m     ConvTranspose1d, ConvTranspose2d, ConvTranspose3d, \\\n\u001b[1;32m      5\u001b[0m     LazyConv1d, LazyConv2d, LazyConv3d, LazyConvTranspose1d, LazyConvTranspose2d, LazyConvTranspose3d\n",
      "File \u001b[0;32m~/Documents/Year3/Computer-Vision-Coursework/coursework_02/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parameter\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, device, dtype\n",
      "File \u001b[0;32m~/Documents/Year3/Computer-Vision-Coursework/coursework_02/venv/lib/python3.10/site-packages/torch/nn/parameter.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _disabled_torch_function_impl\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Metaclass to combine _TensorMeta and the instance check override for Parameter.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch._C'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "# These libraries should be sufficient for this tutorial.\n",
    "# However, if any other library is needed, please install by yourself.\n",
    "import tarfile\n",
    "import imageio.v3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4TX-CXBHW4c"
   },
   "source": [
    "## 1. Download and visualise the imaging dataset.\n",
    "\n",
    "The dataset is curated from the brain imaging dataset in [Medical Decathlon Challenge](http://medicaldecathlon.com/). To save the storage and reduce the computational cost for this tutorial, we extract 2D image slices from T1-Gd contrast enhanced 3D brain volumes and downsample the images.\n",
    "\n",
    "The dataset consists of a training set and a test set. Each image is of dimension 120 x 120, with a corresponding label map of the same dimension. There are four number of classes in the label map:\n",
    "\n",
    "- 0: background\n",
    "- 1: edema\n",
    "- 2: non-enhancing tumour\n",
    "- 3: enhancing tumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mt93oQ8xZkE9"
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!wget https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
    "\n",
    "# Unzip the '.tar.gz' file to the current directory\n",
    "datafile = tarfile.open('Task01_BrainTumour_2D.tar.gz')\n",
    "datafile.extractall()\n",
    "datafile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu_BTL0x6o5a"
   },
   "source": [
    "Suggested colour map for brain MR image:\n",
    "```\n",
    "cmap = 'gray'\n",
    "```\n",
    "\n",
    "Suggested colour map for segmentation map:\n",
    "```\n",
    "cmap = colors.ListedColormap(['black', 'green', 'blue', 'red'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fgubCRC6m4k"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_DIR = Path('./Task01_BrainTumour_2D')\n",
    "\n",
    "TRAINING_IMAGES_DIR = IMAGES_DIR / 'training_images/'\n",
    "TRAINING_LABELS_DIR = IMAGES_DIR / 'training_labels/'\n",
    "\n",
    "def extract_images_from(dir_path):\n",
    "    images = {}\n",
    "    for file in dir_path.iterdir():\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "        images[file.name] = imageio.v3.imread(file)\n",
    "    return images\n",
    "        \n",
    "training_images_dict = extract_images_from(TRAINING_IMAGES_DIR)\n",
    "training_labels_dict = extract_images_from(TRAINING_LABELS_DIR)\n",
    "\n",
    "def zip_images_and_labels(image_dict, label_dict):\n",
    "    pairs = []\n",
    "    for file_name in image_dict.keys():\n",
    "        pairs.append((image_dict[file_name], label_dict[file_name]))\n",
    "    return pairs\n",
    "\n",
    "images_with_labels = zip_images_and_labels(training_images_dict, training_labels_dict)\n",
    "    \n",
    "NUMBER_OF_IMAGES_TO_VISUALISE = 4\n",
    "\n",
    "images_with_labels = random.sample(images_with_labels, NUMBER_OF_IMAGES_TO_VISUALISE)\n",
    "\n",
    "# For visualising the labels, the following color map was used\n",
    "# 0 -> 'black' : background\n",
    "# 1 -> 'green' : edema\n",
    "# 2 -> 'blue'  : non-enhancing tumour\n",
    "# 3 -> 'red'   : enhancing tumour\n",
    "def visualise(images_with_labels):\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    columns = 2 # In each row we visualise the image and its labels.\n",
    "    rows = len(images_with_labels)\n",
    "    for i, (image, labels) in enumerate(images_with_labels):\n",
    "        figure.add_subplot(rows, columns, 2*i + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.gcf().set_size_inches(8, 8)\n",
    "        figure.add_subplot(rows, columns, 2*i + 2)\n",
    "        plt.imshow(labels, cmap=colors.ListedColormap(['black', 'green', 'blue', 'red']))\n",
    "        plt.gcf().set_size_inches(8, 8)\n",
    "        \n",
    "    plt.show()\n",
    "        \n",
    "visualise(images_with_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xWGT3KaML-D"
   },
   "source": [
    "## 2. Implement a dataset class.\n",
    "\n",
    "It can read the imaging dataset and get items, pairs of images and label maps, as training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6p6wFZ3na5z9"
   },
   "outputs": [],
   "source": [
    "def normalise_intensity(image, thres_roi=1.0):\n",
    "    \"\"\" Normalise the image intensity by the mean and standard deviation \"\"\"\n",
    "    # ROI defines the image foreground\n",
    "    val_l = np.percentile(image, thres_roi)\n",
    "    roi = (image >= val_l)\n",
    "    mu, sigma = np.mean(image[roi]), np.std(image[roi])\n",
    "    eps = 1e-6\n",
    "    image2 = (image - mu) / (sigma + eps)\n",
    "    return image2\n",
    "\n",
    "\n",
    "class BrainImageSet(Dataset):\n",
    "    \"\"\" Brain image set \"\"\"\n",
    "    def __init__(self, image_path, label_path='', deploy=False):\n",
    "        self.image_path = Path(image_path)\n",
    "        self.label_path = Path(label_path)\n",
    "        self.deploy = deploy\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        images_dict = extract_images_from(self.image_path)\n",
    "        \n",
    "        # Swapped the code for reading in images because the imageio.v3 library\n",
    "        # was complaining about it.\n",
    "        if not self.deploy:\n",
    "            labels_dict = extract_images_from(self.label_path)\n",
    "            images_with_labels = zip_images_and_labels(images_dict, labels_dict)\n",
    "            images, labels = list(zip(*images_with_labels))\n",
    "            # Needed to reshape the array here so that pytorch deals with it correctly.\n",
    "            self.images = np.array([np.reshape(image, (1, 120, 120)) for image in images])\n",
    "            self.labels = np.array([np.reshape(label, (1, 120, 120)) for label in labels])\n",
    "        else:\n",
    "            self.images = images.values()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get an image and perform intensity normalisation\n",
    "        # Dimension: XY\n",
    "        image = normalise_intensity(self.images[idx])\n",
    "\n",
    "        # Get its label map\n",
    "        # Dimension: XY\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "    def get_random_batch(self, batch_size):\n",
    "        # Get a batch of paired images and label maps\n",
    "        # Dimension of images: NCXY\n",
    "        # Dimension of labels: NXY\n",
    "        images, labels = [], []\n",
    "\n",
    "        random_indices = random.sample(range(len(self)), batch_size)\n",
    "        \n",
    "        for idx in random_indices:\n",
    "            image, label = self[idx]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa4ZpawDNmwu"
   },
   "source": [
    "## 3. Build a U-net architecture.\n",
    "\n",
    "You will implement a U-net architecture. If you are not familiar with U-net, please read this paper:\n",
    "\n",
    "[1] Olaf Ronneberger et al. [U-Net: Convolutional networks for biomedical image segmentation](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28). MICCAI, 2015.\n",
    "\n",
    "For the first convolutional layer, you can start with 16 filters. We have implemented the encoder path. Please complete the decoder path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMPmBZVGb1aI"
   },
   "outputs": [],
   "source": [
    "\"\"\" U-net \"\"\"\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channel=1, output_channel=1, num_filter=16):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # BatchNorm: by default during training this layer keeps running estimates\n",
    "        # of its computed mean and variance, which are then used for normalization\n",
    "        # during evaluation.\n",
    "\n",
    "        # Encoder path\n",
    "        n = num_filter  # 16\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        n *= 2  # 32\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        n *= 2  # 64\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        n *= 2  # 128\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # The first step in each one of conv2 conv3 conv4 above is the max pooling\n",
    "        # for the decode step we need to perform up-conv 2x2 and then the rest\n",
    "        # of the structure remains the same (BatchNorm2d -> ReLU -> Conv2d -> BatchNorm2d -> ReLU)\n",
    "        \n",
    "        # Each of the decode steps also needs to take the skipped input.\n",
    "        # In the paper it was stated that they used an up-convolution using a \n",
    "        # learned kernel. It means that we need to use ConvTranspose2d to have\n",
    "        # a trainable weight on that layer (rather than using e.g. a simple upsampling).\n",
    "        \n",
    "        # In the following we need to split the layers as the U-Net takes the skipped\n",
    "        # outputs from the previous convolution layers and concatenates them with \n",
    "        # the intermediate steps. Therefore we need to perform upconvolution and the \n",
    "        # regular conv 3x3 ReLU steps separately.\n",
    "        \n",
    "        # Decoder path\n",
    "        # We decrease the number of output channels.\n",
    "        n = int(n / 2) # 64\n",
    "        self.upconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(n * 2, n, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # This one takes in the concatenated input therefore it will still have n*2 input channels.\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(n*2, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        n = int(n / 2) # 32\n",
    "        self.upconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(n * 2, n, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(n*2, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        n = int(n / 2) # 16\n",
    "        self.upconv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(n * 2, n, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # For the output layer we need conv 1x1 as described in the paper\n",
    "        NUMBER_OF_CLASSES = 4 # Need a separate output for each class.\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(n*2, n, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n, NUMBER_OF_CLASSES, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the convolutional operators defined above to build the U-net\n",
    "        # The encoder part is already done for you.\n",
    "        # You need to complete the decoder part.\n",
    "        # Encoder\n",
    "        x = self.conv1(x)\n",
    "        conv1_skip = x\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        conv2_skip = x\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        conv3_skip = x\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        # Decoder\n",
    "        \n",
    "        # First we perform up-convolution and then concatenate with the skipped values\n",
    "        # Question: does the order of concatenation matter?\n",
    "        # After concatenation we perform the regular conv 3x3 ReLU.\n",
    "        \n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat((conv3_skip, x), 1)\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat((conv2_skip, x), 1)\n",
    "        x = self.conv6(x)\n",
    "        \n",
    "        x = self.upconv3(x)\n",
    "        x = torch.cat((conv1_skip, x), 1)\n",
    "        x = self.conv7(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcNWZS08d47P"
   },
   "source": [
    "## 4. Train the segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaGGkKQndIaR"
   },
   "outputs": [],
   "source": [
    "# CUDA device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {0}'.format(device))\n",
    "\n",
    "# Build the model\n",
    "num_class = 4\n",
    "model = UNet(input_channel=1, output_channel=num_class, num_filter=16)\n",
    "model = model.to(device)\n",
    "params = list(model.parameters())\n",
    "\n",
    "model_dir = 'saved_models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(params, lr=1e-3)\n",
    "\n",
    "# Segmentation loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Datasets\n",
    "train_set = BrainImageSet('./Task01_BrainTumour_2D/training_images/', './Task01_BrainTumour_2D/training_labels/')\n",
    "test_set = BrainImageSet('./Task01_BrainTumour_2D/test_images/', './Task01_BrainTumour_2D/test_labels/')\n",
    "\n",
    "# Train the model\n",
    "# Note: when you debug the model, you may reduce the number of iterations or batch size to save time.\n",
    "num_iter = 10000\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 16\n",
    "start = time.time()\n",
    "\n",
    "for it in range(1, 1 + num_iter):\n",
    "    # Set the modules in training mode, which will have effects on certain modules, e.g. dropout or batchnorm.\n",
    "    start_iter = time.time()\n",
    "    model.train()\n",
    "\n",
    "    # Get a batch of images and labels\n",
    "    images, labels = train_set.get_random_batch(train_batch_size)\n",
    "    images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
    "    images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "\n",
    "    # Perform optimisation and print out the training loss\n",
    "    optimizer.zero_grad()\n",
    "    output = model(images)\n",
    "    loss = criterion(torch.squeeze(output), torch.squeeze(labels))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # As instructed on edStem, the printing while training is disabled.\n",
    "    #print(\"Iteration: {}, Training loss: {}\".format(it, loss.item()))\n",
    "\n",
    "    # Evaluate\n",
    "    if it % 100 == 0:\n",
    "        model.eval()\n",
    "        # Disabling gradient calculation during reference to reduce memory consumption\n",
    "        with torch.no_grad():\n",
    "            # Evaluate on a batch of test images and print out the test loss\n",
    "            images, labels = test_set.get_random_batch(train_batch_size)\n",
    "            images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
    "            images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "            output = model(images)\n",
    "            loss = criterion(torch.squeeze(output), torch.squeeze(labels))\n",
    "            print(\"Iteration: {}, Test loss: {}\".format(it, loss.item()))\n",
    " \n",
    "\n",
    "    # Save the model\n",
    "    if it % 1000 == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(model_dir, 'model_{0}.pt'.format(it)))\n",
    "print('Training took {:.3f}s in total.'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89yjxjGyb6yT"
   },
   "source": [
    "## 5. Deploy the trained model to a random set of 4 test images and visualise the automated segmentation.\n",
    "\n",
    "You can show the images as a 4 x 3 panel. Each row shows one example, with the 3 columns being the test image, automated segmentation and ground truth segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZeLE0qZjd2j"
   },
   "outputs": [],
   "source": [
    "images, labels = test_set.get_random_batch(4)\n",
    "images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n",
    "images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, 'model_10000.pt')))\n",
    "\n",
    "output = model(images)\n",
    "\n",
    "def visualise(images, labels, output):\n",
    "    with torch.no_grad():\n",
    "        images = torch.squeeze(images).detach()\n",
    "        labels = torch.squeeze(labels).detach()\n",
    "        # Need to produce a single image from output\n",
    "        outputs = torch.squeeze(output).detach()\n",
    "        merged_outputs = []\n",
    "        for output in outputs:\n",
    "            # apply thresholding so that the black one doesn't always win\n",
    "            merged_outputs.append(np.argmax(output, axis = 0))\n",
    "            \n",
    "        figure = plt.figure(figsize=(8, 8))\n",
    "        columns = 3 \n",
    "        rows = 4\n",
    "        plt.figtext(0.5, 0.05, '3 columns represent: test image, automated segmentation and ground truth segmentation in order.', horizontalalignment='center',\n",
    "             verticalalignment='top')\n",
    "        for i, (image, label, output) in enumerate(zip(images, labels, merged_outputs)):\n",
    "            figure.add_subplot(rows, columns, 3*i + 1)\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.gcf().set_size_inches(8, 8)\n",
    "            figure.add_subplot(rows, columns, 3*i + 3)\n",
    "            plt.imshow(output, cmap=colors.ListedColormap(['black', 'green', 'blue', 'red']))\n",
    "            plt.gcf().set_size_inches(8, 8)\n",
    "            figure.add_subplot(rows, columns, 3*i + 2)\n",
    "            plt.imshow(label, cmap=colors.ListedColormap(['black', 'green', 'blue', 'red']))\n",
    "            plt.gcf().set_size_inches(8, 8)\n",
    "            \n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "loss_after_iterations = []\n",
    "for i in range(1, 11):\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, 'model_{}000.pt'.format(i))))\n",
    "    print(\"Model produced after {}000 iterations:\".format(i))\n",
    "    loss = criterion(torch.squeeze(output), torch.squeeze(labels))\n",
    "    print(\"Test loss: {}\".format(loss.item()))\n",
    "    loss_after_iterations.append(loss.item())\n",
    "\n",
    "    output = model(images)\n",
    "    visualise(images, labels, output)\n",
    "    \n",
    "x = np.array([\"{}000\".format(i) for i in range(1, 11)])\n",
    "y = np.array(loss_after_iterations)\n",
    "\n",
    "print(\"Test error for each 1000 of iterations:\")\n",
    "plt.plot(x, y)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cj3Qusin_s_r"
   },
   "source": [
    "## 6. Discussion. Does your trained model work well? How would you improve this model so it can be deployed to the real clinic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVwEtDKIdTRs"
   },
   "source": [
    "As it could be observed in the output image segmentation above, the model gives automated segmentation images which visually resemble the ground truth values.\n",
    "One thing I've noticed is that the test error sometimes doesn't fully resemble the visual correspondence of the images. For example in the outputs generated for the \n",
    "model after 8000 iterations have the lowest test error out of all models (see the graph above) however for the first picture the whole tumour is marked with red and it should be blue if we examine the ground truth.\n",
    "We can contrast that with the output after 10000 iterations, where the tumour is correctly labeled in blue, however the overall error is larger. \n",
    "\n",
    "This inconsistency could be attributed to the fact that we compute the loss over the whole batch of 4 images and it could be that the model performed worse on the\n",
    "other images in that case. \n",
    "\n",
    "Another thing to notice is the overfitting present in the model. One could observe by looking at the training output above that the training error keept decreasing as I was training the model, \n",
    "however the test error seems not to be affected after some point. That could also be seen in the plot above, where the performance after 9000 iterations is worse than after 8. \n",
    "It is important to note however, that all of those scores were computed only for small batches of images and so in order to accurately compare the performance of the models we would have to do it for the whole test dataset. \n",
    "\n",
    "Overall I think the trained models perform well provided that they are trained for at least 4000 iterations. Both the test scores computed using the cross-entropy loss and the visual resemblance are quite good.\n",
    "\n",
    "In order to improve the model so that it can be deployed in the real clinic I would probably consider performing some more advanced hyperparameter tuning to improve the performance. One could adapt the learning rate for the Adam optimiser used in the network for example. I could also try to tweak the size of a training batch. Before using the model in a production environment, I would probably perform some kind of exhaustive grid search to find the best hyperparameters to pick when training the model.\n",
    "\n",
    "I would also consider using a larger dataset to train the model. If the data is not available, we could introduce some transformations to the images to artificially increase the number of training data (e.g. some deformations or smooth stretching). \n",
    "\n",
    "If the model were to be used in an actual medical setting, it needs to be very robust as human life is at stake. Especially false negatives could be very dangerous. The model should be able to reliably detect presence of some kind of \n",
    "tumour, and its actual type has lower priority. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
